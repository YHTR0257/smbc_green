notebooks:
  - exploratory.ipynb
  - feature_engineering.ipynb
  - model_prototyping.ipynb

src:
  data_processing.py:
    functions:
      - load_data
      - clean_data
      - transform_data
  models.py:
    classes:
      - ModelArchitecture
  training.py:
    functions:
      - train_model
      - evaluate_model
  pipelines.py:
    functions:
      - data_pipeline
      - training_pipeline
  utils.py:
    functions:
      - save_model
      - load_model

data_path:
  raw_data: "data/raw/"
  processed_data: "data/processed/"
  reports: "reports/"
  submits: "data/submissions/"
  model_checkpoints: "models/checkpoints/"
  experiments: "experiments/"
  logs: "logs/"

train_config:
  general:
    target: "price_actual"
    drop_columns: ["valencia_snow_3h", "madrid_snow_3h", "bilbao_snow_3h", "barcelona_snow_3h", "seville_snow_3h"]
    test_size: 0.2
    validation_size: 0.1
    random_seed: [42, 123, 456]
    log_level: "INFO"
    # Time-based split configuration
    time_column: "time"
    train_years: [2015, 2016]
    validation_year: 2017
    use_time_split: true
  random_forest:
    n_estimators: [100, 200]
    max_depth: [10, 20]
    min_samples_split: [2, 5]
    min_samples_leaf: [1, 2]
  neural_network:
    input_size: [64, 128]
    hidden_layers: [[128, 64], [256, 128]]
    output_size: [10, 20]
    activation_function: ["relu", "tanh"]
    dropout_rate: [0.5, 0.3]
  denoising_autoencoder:
    input_size: [64, 128]
    hidden_layers: [[128, 64], [256, 128]]
    output_size: [10, 20]
    activation_function: ["relu", "tanh"]
    dropout_rate: [0.5, 0.3]
  xgboost:
    objective: reg:squarederror
    # GPU/CPU configuration
    use_gpu: auto             # auto, true, false
    tree_method_gpu: gpu_hist # GPU tree method
    tree_method_cpu: hist     # CPU tree method
    predictor_gpu: gpu_predictor # GPU predictor
    predictor_cpu: auto       # CPU predictor
    max_depth: 6
    eta: 0.1
    eval_metric: rmse
    num_boost_round: 1000
    subsample: 0.8
    colsample_bytree: 0.8
    early_stopping_rounds: 50
    verbose_eval: 100
    # Validation settings
    use_validation: true
    validation_metric: "rmse"
    # Optuna hyperparameter optimization
    optuna:
      enabled: true
      n_trials: 100
      timeout: 3600  # seconds (1 hour)
      direction: minimize  # minimize RMSE
      pruner: median  # median, successive_halving, hyperband
      study_name: "xgboost_optimization"
      storage: null  # Use in-memory storage, can be set to sqlite:///optuna.db
      # Hyperparameter search spaces
      search_space:
        max_depth: [3, 10]  # int range
        eta: [0.01, 0.3]    # float range
        subsample: [0.6, 1.0]  # float range
        colsample_bytree: [0.6, 1.0]  # float range
        min_child_weight: [1, 10]  # int range
        gamma: [0, 5]  # float range
        reg_alpha: [0, 10]  # float range
        reg_lambda: [1, 10]  # float range
  

experiments:
  - experiment_20250602_001/
  - experiment_20250602_002/

models:
  - best_model_v1.pth
  - latest_model.pth

reports:
  - final_report.pdf
  - visualizations/

tests:
  - test_data_processing.py
  - test_models.py

README.md:
  title: "Data Science Project"
  description: "This project aims to..."