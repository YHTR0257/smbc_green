notebooks:
  - exploratory.ipynb
  - feature_engineering.ipynb
  - model_prototyping.ipynb

src:
  data_processing.py:
    functions:
      - load_data
      - clean_data
      - transform_data
  models.py:
    classes:
      - ModelArchitecture
  training.py:
    functions:
      - train_model
      - evaluate_model
  pipelines.py:
    functions:
      - data_pipeline
      - training_pipeline
  utils.py:
    functions:
      - save_model
      - load_model

data_path:
  raw_data: "data/raw/"
  processed_data: "data/processed/"
  reports: "reports/"
  submits: "data/submissions/"
  model_checkpoints: "models/checkpoints/"
  experiments: "experiments/"
  logs: "logs/"

train_config:
  general:
    target: "price_actual"
    drop_columns: ["valencia_snow_3h", "madrid_snow_3h", "bilbao_snow_3h", "barcelona_snow_3h", "seville_snow_3h"]
    test_size: 0.2
    validation_size: 0.1
    random_seed: [42, 123, 456]
    log_level: "INFO"
    # Time-based split configuration
    time_column: "time"
    train_years: [2015, 2016]
    validation_year: 2017
    use_time_split: true
  random_forest:
    n_estimators: [100, 200]
    max_depth: [10, 20]
    min_samples_split: [2, 5]
    min_samples_leaf: [1, 2]
  neural_network:
    input_size: [64, 128]
    hidden_layers: [[128, 64], [256, 128]]
    output_size: [10, 20]
    activation_function: ["relu", "tanh"]
    dropout_rate: [0.5, 0.3]
  lstm:
    # GPU/CPU configuration
    use_gpu: auto              # auto, true, false
    # Model architecture
    hidden_size: 64            # LSTM hidden size
    num_layers: 2              # Number of LSTM layers
    output_size: 1             # Output size (1 for regression)
    dropout: 0.1               # Dropout rate
    bidirectional: false       # Use bidirectional LSTM
    # Training parameters
    sequence_length: 24        # Length of input sequences (hours)
    batch_size: 32            # Batch size for training
    epochs: 100               # Number of training epochs
    learning_rate: 0.001      # Learning rate for optimizer
    # Validation settings
    use_validation: true
    early_stopping_patience: 10
  denoising_autoencoder:
    input_size: [64, 128]
    hidden_layers: [[128, 64], [256, 128]]
    output_size: [10, 20]
    activation_function: ["relu", "tanh"]
    dropout_rate: [0.5, 0.3]
  lightgbm:
    objective: regression
    # GPU/CPU configuration
    use_gpu: auto             # auto, true, false
    max_depth: 6
    learning_rate: 0.1
    metric: rmse
    num_boost_round: 1000
    bagging_fraction: 0.8
    feature_fraction: 0.8
    early_stopping_rounds: 50
    verbose_eval: 100
    # Validation settings
    use_validation: true
    validation_metric: "rmse"
    # Optuna hyperparameter optimization
    optuna:
      enabled: true
      n_trials: 100
      timeout: 3600  # seconds (1 hour)
      direction: minimize  # minimize RMSE
      pruner: median  # median, successive_halving, hyperband
      study_name: "lightgbm_optimization"
      storage: null  # Use in-memory storage, can be set to sqlite:///optuna.db
      # Hyperparameter search spaces
      search_space:
        max_depth: [3, 10]  # int range
        learning_rate: [0.01, 0.3]    # float range
        bagging_fraction: [0.6, 1.0]  # float range
        feature_fraction: [0.6, 1.0]  # float range
        num_leaves: [10, 300]  # int range
        min_child_weight: [1, 10]  # int range
        reg_alpha: [0, 10]  # float range
        reg_lambda: [1, 10]  # float range

# Feature selection configuration
feature_selection:
  enabled: true
  lasso_cv_folds: 5
  max_features: 100

# Ensemble configuration
ensemble:
  enabled: true             # Enable ensemble training
  method: weighted_average    # average, weighted_average, median
  optimize_weights: true      # Optimize ensemble weights using validation data
  models:
    - type: lightgbm
      name: lgb_diverse
      seeds: [42, 123, 456]    # Multiple seeds for diversity
    - type: lstm  
      name: lstm_diverse
      seeds: [42, 123]         # Multiple LSTM models with different seeds
  # Weight optimization settings
  weight_optimization:
    method: scipy_minimize     # scipy_minimize, grid_search
    metric: rmse              # Metric to optimize

experiments:
  - experiment_20250602_001/
  - experiment_20250602_002/

models:
  - best_model_v1.pth
  - latest_model.pth

reports:
  - final_report.pdf
  - visualizations/

tests:
  - test_data_processing.py
  - test_models.py

README.md:
  title: "Data Science Project"
  description: "This project aims to..."